{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile processing Step 1: Pre-processing CTD, identification of data groups, and image tags\n",
    "\n",
    "works with the raw CTD time series, Castaway processed CTD profiles, and all of the images in the folder to sync the time and create files that will be used in the next step to associate a time, depth, salinity, and temperature with every particle identified by the image processing output. It is in this notebook that you will also select specific intervals within the cast to process (if desired) and identify the total depth (which is needed later on in processing the profile data). Three files are required to complete this step of the processing. They are:\n",
    "\n",
    "1) raw CTD time series\n",
    "2) Castaway processed CTD profiles\n",
    "3) The images to process or the images that were processed \n",
    "\n",
    "This notebook can be run before or after the image processing. \n",
    "\n",
    "The user input needed in the notebook includes:\n",
    "- specifying the cast to process through the file ```0_CastPath.csv``` (this file needs to be in the same directory as the step 1 notebook)\n",
    "- any time corrections to match the images (computer clock) and CTD (CTD clock)\n",
    "- the image file type (e.g., jpg, bmp, etc.) and the file type of the CTD data (should be .csv)\n",
    "\n",
    "During the execution of the notebook, you will need to identify whether or not there are breakpoints you want to associate with the cast to group the data and how you want to define the depth. Completing these should be self-explanatory as you work through the file.\n",
    "\n",
    "Files generated through the execution of this notebook include:\n",
    "\n",
    "- CTD-profile.csv (a renamed copy for easy read in later on)\n",
    "- CTD-timeseries.csv (a renamed copy for easy read in later on)\n",
    "- Data-Breakpionts.csv (a list unique set of data groupings for the cast)\n",
    "- Depth.csv (depth to be used in later profiling)\n",
    "- ImageTime.csv (list of image day/times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python loads and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina' # hig-res plots for a Retina display \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import shutil\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define equation for converting conductivity to specific conductance\n",
    "#C - conductivity in microsiemens/cm\n",
    "#T - temperature in celcius\n",
    "\n",
    "def SpC(C,T):\n",
    "    return C/(1+0.02*(T-25))\n",
    "    \n",
    "    \n",
    "#returns the clear-water density in kg/m^3 as a function of temperature [in deg C] and salinity [ppt]\n",
    "def rho_cw(T,S): \n",
    "    rho_fresh=1000*(1-(T +288.9414)/(508929.2*(T+68.12963))*(T-3.9863)**2)\n",
    "    Acoef = 0.824493 - 0.0040899*T + 0.000076438*T**2 -0.00000082467*T**3 + 0.0000000053675*T**4\n",
    "    Bcoef = -0.005724 + 0.00010227*T - 0.0000016546*T**2\n",
    "    return rho_fresh + Acoef*S + Bcoef*S**(3/2) + 0.00048314*S**2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find files and enter time corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" --- User input ----------------------------- \"\"\"\n",
    "# file types and paths\n",
    "filetype = '*.csv'\n",
    "imagetype = '*jpg'\n",
    "\n",
    "# time corrections to be added onto CTD to match Camera stamp (use negative number to substract)\n",
    "hrC = 0  # hour correction\n",
    "minC = 0 # min correction \n",
    "scC = 4   # second correction (Barateria Bay correction)\n",
    "# scC = 0   # second correction (Siene correction)\n",
    "\"\"\"  ------------------------------------------- \"\"\"\n",
    "\n",
    "# castpath = '/Users/strom-adm/Documents/Floc-Processing/Code/1_Profile_Processing/0_CastPath.csv'\n",
    "castpath = '0_CastPath.csv'\n",
    "Path = pd.read_csv(castpath).profile_path[0]+'/'\n",
    "\n",
    "CodePath = os.getcwd()\n",
    "os.chdir(Path)\n",
    "\n",
    "# find the data files  \n",
    "\n",
    "files = sorted(glob.glob(filetype))\n",
    "\n",
    "for i in range(0,len(files)):\n",
    "    print('file: ',files[i],'(index: ',i,')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick files for CTD processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" --- User input ----------------------------- \"\"\"\n",
    "m = 0 # pick the processed profile data by choosing the file index\n",
    "n = 1 # pick the raw CTD time series by choosing the file index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process CTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a copy of the profile csv so that it is easily read in when creating the super data\n",
    "\n",
    "if pd.read_csv(files[m],  nrows=1).shape[1] == 2:\n",
    "    ctd_profile = pd.read_csv(files[m], skiprows = 28)\n",
    "    ctd_profile.to_csv('CTD-profile.csv',index = False)\n",
    "else:\n",
    "    shutil.copyfile(files[m], 'CTD-profile.csv')\n",
    "\n",
    "# get the ctd file, rearrange it and make it into a dataframe \n",
    "\n",
    "ctd_data = pd.read_csv(files[n], skiprows = 28)\n",
    "ctd_header = pd.read_csv(files[n], nrows = 27)\n",
    "ctd = pd.concat([ctd_data, ctd_header], axis=1)\n",
    "\n",
    "# associated a full corrected day and time stamp with each time series measurement\n",
    "\n",
    "cor_time = ctd.iloc[2,5]\n",
    "cor_time = datetime.strptime(cor_time,'%Y-%m-%d %H:%M:%S')\n",
    "cor_time = cor_time + timedelta(hours=hrC,minutes=minC,seconds=scC)    ##Change time to CST\n",
    "\n",
    "N = len(ctd.iloc[:,0])\n",
    "cor_timess = np.empty(N, dtype=object)\n",
    "for i in np.arange(0,N):\n",
    "    cor_times = cor_time + timedelta(seconds = ctd.iloc[i,0])\n",
    "    cor_timess[i] = cor_times.strftime(\"%m%d%Y%H%M%S\") \n",
    "    \n",
    "# average P,T,C from CTD data, calculate specific conductance from C and convert P to depth\n",
    "\n",
    "avg_ctd_P = np.zeros(N)\n",
    "avg_ctd_T = np.zeros(N)\n",
    "avg_ctd_C = np.zeros(N)\n",
    "for i in np.arange(2,N-2):\n",
    "    avg_ctd_P[i] = np.average([ctd.iloc[i-2,1],ctd.iloc[i-1,1],ctd.iloc[i,1],ctd.iloc[i+1,1],ctd.iloc[i+2,1]])\n",
    "    avg_ctd_T[i] = np.average([ctd.iloc[i-2,2],ctd.iloc[i-1,2],ctd.iloc[i,2],ctd.iloc[i+1,2],ctd.iloc[i+2,2]])\n",
    "    avg_ctd_C[i] = np.average([ctd.iloc[i-2,3],ctd.iloc[i-1,3],ctd.iloc[i,3],ctd.iloc[i+1,3],ctd.iloc[i+2,3]])\n",
    "    \n",
    "avg_SpC = SpC(avg_ctd_C,avg_ctd_T)      #microsiemens/cm\n",
    "depth = avg_ctd_P*10/9.81               #meters \n",
    "\n",
    "# insert data into ctd dataframe and save as a csv file\n",
    "\n",
    "ctd.insert(0,'Time',cor_timess)\n",
    "ctd.insert(1,'Depth [m]',depth)\n",
    "ctd.insert(2,'P [Decibars]',avg_ctd_P)\n",
    "ctd.insert(3,'T [Celsius]',avg_ctd_T)\n",
    "ctd.insert(4,'SpC [MicroSiemens/cm]',avg_ctd_C)\n",
    "\n",
    "ctd.to_csv('CTD-timeseries.csv',index = False)\n",
    "\n",
    "display(ctd)\n",
    "\n",
    "# auto set the max detph\n",
    "maxdepth = max(ctd['Depth [m]'])\n",
    "\n",
    "# plot the data\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(ctd['Time (Seconds)'], ctd['SpC [MicroSiemens/cm]'], color='C03',alpha=1,label='SpC')\n",
    "ax1.set_ylabel('SpC [MicroSiemens/cm]')\n",
    "# ax1.set_ylim(33000,37000)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(ctd['Time (Seconds)'], ctd['Depth [m]'], alpha=1,label='depth')\n",
    "ax2.set_xlabel('Time [s]')\n",
    "ax2.set_ylabel('Depth [m]')\n",
    "ax2.axhline(y = maxdepth, color = 'k', linestyle = '--', label='max depth')\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(h1+h2, l1+l2, loc=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the break points for specific subsets of the data and the max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" --- User input ----------------------------- \"\"\"\n",
    "subsets = 1 # enter \"1\" to have subsets, zero for none\n",
    "automax = 1 # use \"1\" to just take the max of the measurement. Set automax = 0 to use a user-specified value (then must set manual max below)\n",
    "# manualmax = 12.8 # Set max depth manually\n",
    "\n",
    "# Time breakpoints... enter the start and end of each period or subset for which you want the data grouped (use nearest second)\n",
    "\n",
    "# Seine - 4/17/2022 --------------------------------\n",
    "\n",
    "names = np.array(['profile','surface','mid-depth','bottom'])\n",
    "\n",
    "startS = np.array([62,235,358,458]) # profile 4\n",
    "endS = np.array([234,322,415,528]) # profile 4\n",
    "\n",
    "# # Seine - 6/16/2022 --------------------------------\n",
    "\n",
    "# names = np.array(['profile','surface','mid-depth','bottom'])\n",
    "\n",
    "# startS = np.array([180,338,553,714]) # profile 4\n",
    "# endS = np.array([338,518,679,832]) # profile 4\n",
    "\n",
    "\"\"\"  ------------------------------------------- \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "if automax == 0:\n",
    "    maxdepth = manualmax\n",
    "else:\n",
    "    waterdepth = pd.DataFrame({'Depth [m]':np.array([maxdepth])})\n",
    "    waterdepth.to_csv('Depth.csv',index=False)\n",
    "    \n",
    "if subsets == 1:\n",
    "    \n",
    "    deltaT = endS - startS\n",
    "    start_time = np.zeros(len(startS))\n",
    "    end_time = np.zeros(len(endS))\n",
    "\n",
    "    for i in range(0,len(startS)):\n",
    "        temp = ctd[(ctd['Time (Seconds)'] == startS[i])].copy()\n",
    "        start_time[i] = temp.Time.iloc[0]\n",
    "\n",
    "    for i in range(0,len(endS)):\n",
    "        temp = ctd[(ctd['Time (Seconds)'] == endS[i])].copy()\n",
    "        end_time[i] = temp.Time.iloc[0]\n",
    "\n",
    "    breaktimes = pd.DataFrame({'Type/Location':names,'Start [DateTime]':start_time, 'End [DateTime]':end_time, 'Durration [sec]':deltaT})\n",
    "    breaktimes.to_csv('Data-Breakpionts.csv',index = False)\n",
    "    display(breaktimes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the depth to be used later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Depth [m]':np.array([maxdepth])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract time stamps from images and save as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the list of images, build a dataframe, and then extract the date time\n",
    "\n",
    "# uses the filemod_list.csv file ---------------\n",
    "\n",
    "images = pd.read_csv('filemod_list.csv', usecols = ['file_Name'])\n",
    "\n",
    "N = len(images)\n",
    "datetime_raw = np.empty(N, dtype=object)\n",
    "for i in np.arange(0,N):\n",
    "    datetime_raw[i] = str(images.iloc[i,0].split('-')[1]) #adjust the number in brackets until the datetime_raw is the timestamp. This will depend on how you named the images in the field. \n",
    "images['Image Time']=datetime_raw\n",
    "images.to_csv('ImageTime.csv', index=False)\n",
    "display(images)\n",
    "\n",
    "# uses the actual image files -----------------\n",
    "# Ifiles = sorted(glob.glob(imagetype))\n",
    "# images=pd.DataFrame(Ifiles)\n",
    "\n",
    "# N = len(images)\n",
    "# datetime_raw = np.empty(N, dtype=object)\n",
    "# for i in np.arange(0,N):\n",
    "#     datetime_raw[i] = str(images.iloc[i,0].split('-')[1]) #adjust the number in brackets until the datetime_raw is the timestamp. This will depend on how you named the images in the field. \n",
    "# images['Image Time']=datetime_raw\n",
    "# images.to_csv('ImageTime.csv', index=False)\n",
    "# display(images)\n",
    "\n",
    "# imagehold = images[(images['Image Time'] >= breaktime)].copy()\n",
    "# imagehold.to_csv('ImageTime-hold.csv',index = False)\n",
    "# print(\"Only during the hold ------------\")\n",
    "# display(imagehold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the working directory back to the original code path\n",
    "os.chdir(CodePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
