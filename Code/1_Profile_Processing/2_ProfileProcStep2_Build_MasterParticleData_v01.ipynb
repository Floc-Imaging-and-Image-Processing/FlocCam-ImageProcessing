{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile processing Step 2: Building the master particle size dataset\n",
    "\n",
    "The primary task completed in this profile processing step is to associate a specific date, time, depth, salinity, and temperature with every single particle identified through image processing. The notebook uses files generated in step 1 along with a single master data file of all the particles identified in the cast residing in the folder ```0_analysis_output``` with the cast of interest. The only input needed from the user is to identify the folder path for the cast/profile you wish to analyze and to identify the name of the master particle size file in the ```0_analysis_output``` folder. It should be ```001.csv``` but this may depend on exactly how the processing was completed.\n",
    "\n",
    "The images must have been processed to generate the ```0_analysis_output``` folder and particle data contained within prior to running this notebook. The main output of this processing step is the generation of the file ```particle_profile_data.csv```, which will be saved to the ```0_analysis_output``` folder. ```particle_profile_data.csv``` is identical to the ```001.csv``` in the ```0_analysis_output``` folder with the addition of the depth, salinity, and temperature data added for each identified particle. The ```particle_profile_data.csv``` file is what is used in processing step 3 to look at and output profile data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina' # hig-res plots for a Retina display \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" --- User input ----------------------------- \"\"\"\n",
    "particledata = '/0_analysis_output/001.csv' #dataframe with all of the particle data\n",
    "\"\"\"  ------------------------------------------- \"\"\"\n",
    "\n",
    "# files that should be present in the castpath if step 1 of the processing was completed.\n",
    "# pathfile = '/Users/strom-adm/My Drive/Floc-Processing/Code/1_Profile_Processing/0_CastPath.csv'\n",
    "pathfile = '0_CastPath.csv'\n",
    "castpath = pd.read_csv(pathfile).profile_path[0]+'/'\n",
    "imagetimes = 'ImageTime.csv'\n",
    "ctdtimeseries = 'CTD-timeseries.csv' #path to raw CTD time series data\n",
    "ctdprofile = 'CTD-profile.csv'           #path to CastAway processed time series data\n",
    "depth_file = 'Depth.csv'\n",
    "\n",
    "# read in the data \n",
    "\n",
    "if(os.path.exists(castpath+particledata) == True):\n",
    "    pdata_master = pd.read_csv(castpath+particledata)       \n",
    "    ImageTime_df = pd.read_csv(castpath+imagetimes)  \n",
    "    CTD_df = pd.read_csv(castpath+ctdtimeseries)  \n",
    "    ProcessedCTD = pd.read_csv(castpath+ctdprofile)\n",
    "    totaldepth = pd.read_csv(castpath+depth_file)['Depth [m]'][0]\n",
    "else:\n",
    "    print('Particle data needs to be processed to create folder \"0_analysis_output\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# average ctd data over every second \n",
    "\n",
    "# time  = CTD_df['CTD Time'].unique()\n",
    "avgDepth = np.zeros(len(CTD_df['Time'].unique()))\n",
    "avgTemp = np.zeros(len(CTD_df['Time'].unique()))\n",
    "avgSpC = np.zeros(len(CTD_df['Time'].unique()))\n",
    "avgPSU = np.zeros(len(CTD_df['Time'].unique()))\n",
    "\n",
    "count = 0\n",
    "for time in CTD_df['Time'].unique(): \n",
    "    avgDepth[count] = np.average(CTD_df['Depth [m]'].where(CTD_df['Time'] == time).dropna())\n",
    "    avgTemp[count] = np.average(CTD_df['T [Celsius]'].where(CTD_df['Time'] == time).dropna())\n",
    "    avgSpC[count] = np.average(CTD_df['SpC [MicroSiemens/cm]'].where(CTD_df['Time'] == time).dropna())\n",
    "    count = count + 1\n",
    "\n",
    "#get PSU value from processed CTD data. Currently based on matching conductance in raw to PSU in ctd processed profile\n",
    "\n",
    "count = 0\n",
    "for spc in avgSpC:\n",
    "    indexmatch = (ProcessedCTD['Conductivity (MicroSiemens per Centimeter)']-spc).abs().argsort()[0]\n",
    "    avgPSU[count] = ProcessedCTD.iloc[indexmatch]['Salinity (Practical Salinity Scale)']\n",
    "    count = count+1\n",
    "\n",
    "# create dataframe with average data and map to image_time then map avg data to superfolder \n",
    "\n",
    "time  = CTD_df['Time'].unique()\n",
    "columns = [\"Image Time\",\"Depth [m]\", \"T [Celsius]\", \"SpC [MicroSiemens/cm]\",\"PSU\"]\n",
    "data = np.array([time, avgDepth, avgTemp,avgSpC,avgPSU]).T\n",
    "df_ctd_x = pd.DataFrame(data=data, columns=columns)\n",
    "df_ctd_x = pd.merge(ImageTime_df, df_ctd_x, how='inner', left_on='Image Time', right_on='Image Time')\n",
    "df_ctd_x.rename(columns ={'0': \"Image File\"})\n",
    "\n",
    "#create a matrix \"pre_master\" with average data for each particle to append to pdata_master \n",
    "\n",
    "pre_master = np.zeros((len(pdata_master),5)) # number of particles in master datafile x 5 for the new columns\n",
    "partcount = 0\n",
    "\n",
    "# figure out which is longer, the CTD or images. Use the shorter of the two. If images are longer, the last identified particles will not have CTD info\n",
    "if len(ImageTime_df)>len(df_ctd_x):\n",
    "    end = len(df_ctd_x)\n",
    "else:\n",
    "    end = len(ImageTime_df)\n",
    "\n",
    "for i in np.arange(0,end): # for in range of 0 to the last image in the series can be larger than unique seconds\n",
    "    df_ctd_y = np.array(df_ctd_x.iloc[i,1:]) # the data from df_ctd_x associated with the image name/time\n",
    "    NoParticles = len(pdata_master[(pdata_master['ImgNo'] == i+1)]) #number of particles in image\n",
    "    \n",
    "    pre_master[partcount:partcount+NoParticles,:] = df_ctd_y\n",
    "    partcount = partcount + NoParticles\n",
    "    \n",
    "pre_master_df = pd.DataFrame(data=pre_master, columns=columns)\n",
    "\n",
    "pdata = pd.concat([pdata_master, pre_master_df], axis=1)\n",
    "\n",
    "pdata.insert(loc=22, column='z [m]', value=totaldepth - pdata['Depth [m]'])\n",
    "\n",
    "pdata.to_csv(os.path.join(castpath,'0_analysis_output/particle_profile_data.csv' ),index=False)\n",
    "\n",
    "pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
