{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile processing Step 3a: Looking at the grouped data \n",
    "\n",
    "There are two ways to look at and process the master dataset created in step 2, i.e. the file ```0_analysis_output/particle_profile_data.csv```. The first is to do it by groups identified in step 1 (what is done in this step 2a notebook) The second is to group the data by specified depth bins or particle number bins (what is done in the Step 3b notebook).\n",
    "\n",
    "The group breakpoints used in this notebook are either \n",
    "1. no breakpoints at all (that is, process all the data into a single group); or \n",
    "2. through the use of the cast-specific user-identified breakpoints in step 1. These breaks may be locations where the camera was held steady at a given depth for some amount of time, or they may simply be groups of images for time or depth-averaged calculations. \n",
    "\n",
    "To look at the data, two primary types of input are needed. The first is just the path to the cast of interest. The second type of user input pertains to values needed to convert the particle data from pixel-based measurements to micron-based measurements and the development of particle size distribution statistics. \n",
    "\n",
    "Several types of files will be generated when running the cells of this notebook. The data types include:\n",
    "- Summary statistics for each group (\"ProcData_0_GroupSummary.csv\")\n",
    "- the time series data for each group (\"ProcData_1_timeseries_GroupName.csv\")\n",
    "- the PDF or histogram of each group (\"ProcData_2_PDF_GroupName.csv\")\n",
    "- the CDF of particle size for each group (\"ProcData_2_CDF_GroupName.csv\")\n",
    "- Plot of the profile (ProcData_3_figure-GroupSummary.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina' # hig-res plots for a Retina display \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" --- User input ----------------------------- \"\"\"\n",
    "\n",
    "# folder/file paths\n",
    "# lisstbins_file = '/Users/strom-adm/Floc-Processing/Code/LISST_bins/LISST_bins_random_ext.csv' # LISST bin data if used\n",
    "lisstbins_file = '../LISST_bins/LISST_bins_random_ext.csv'\n",
    "\n",
    "# user-set parameters for processing the data\n",
    "muperpix = 0.925 # muperpix is the number of microns per per pixel (1.28 is equal to 800 pix per mm)\n",
    "darea = 1 # use 1 to base particle size on area, 0 to base it on the minor axis of the fit elips\n",
    "useLISST = 1  # use 1 if list bins sizes are to be used\n",
    "vdist = 1 # use particle vol for distributions rather than the frequency weighting... if 1, then the w value below does not matter, if 0, w value is used\n",
    "nb = 30\n",
    "w = 3 # the distribution weighting value 0 = by number, 1 = by diameter (Ali's), 2 = by area, 3 = by vol\n",
    "\n",
    "\"\"\"  ------------------------------------------- \"\"\"\n",
    "\n",
    "# files to use... should not need to change\n",
    "castpath = pd.read_csv('0_CastPath.csv').profile_path[0]+'/'\n",
    "pdata_file = '0_analysis_output/particle_profile_data.csv' #dataframe with all of the particle data\n",
    "breakpoint_file = 'Data-Breakpionts.csv'\n",
    "depth_file = 'Depth.csv'\n",
    "\n",
    "# read in the file\n",
    "pdata = pd.read_csv(castpath+pdata_file)       \n",
    "totaldepth = pd.read_csv(castpath+depth_file)['Depth [m]'][0]\n",
    "\n",
    "# take a look at the break points\n",
    "\n",
    "if(os.path.exists(castpath+breakpoint_file) == True):\n",
    "    breakpoints = pd.read_csv(castpath+breakpoint_file)\n",
    "    print(\"Here are the breakpoints in the profile.\")\n",
    "    display(breakpoints)\n",
    "else:\n",
    "    print('No breakpoint file found, will process all data as a single profile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at a data snapshot from one of the data group types or locations listed in the breakpoint file\n",
    "\n",
    "This step is not needed for processing. It's just here to allow for quick inspection of the data as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" --- User input ----------------------------- \"\"\"\n",
    "k = 1 # pick the index of the type/location you would like to look at\n",
    "\"\"\"  ------------------------------------------- \"\"\"\n",
    "\n",
    "pdata_group = pdata[(pdata['Image Time'] >= breakpoints['Start [DateTime]'][k]) &\n",
    "     (pdata['Image Time'] <= breakpoints['End [DateTime]'][k])].copy()\n",
    "\n",
    "print('Location: ', breakpoints['Type/Location'][k]) \n",
    "print('- Distance off bed z [m] =',np.around(np.mean(pdata_group['z [m]']),decimals=2))\n",
    "print('- Depth [m] =', np.around(np.mean(pdata_group['Depth [m]']),decimals=2))\n",
    "print('- durration [sec]:',breakpoints['Durration [sec]'][k])\n",
    "print('- number of unique images:', len(pdata_group['ImgNo'].unique())) \n",
    "print('- number of particles:', len(pdata_group['Number']))\n",
    "display(pdata_group)\n",
    "display(breakpoints) # display again to make it easy to select the breakpoints in the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process and summarize an individual or set of subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" --- User input ----------------------------- \"\"\"\n",
    "summaryindexS = 1 # enter the index associated with the first group break to process\n",
    "summaryindexE = 3 # enter the index associated with the last group break to process; \n",
    "                  # set summaryindexE = summaryindexS to process a single group\n",
    "\"\"\"  ------------------------------------------- \"\"\"\n",
    "\n",
    "# start the summary table \n",
    "\n",
    "summary = breakpoints[summaryindexS:summaryindexE+1].copy()\n",
    "summary.index = range(len(summary))\n",
    "\n",
    "# setup summary arrays\n",
    "\n",
    "N=len(summary)\n",
    "depth = np.zeros(N)\n",
    "zbed = np.zeros(N)\n",
    "salinity = np.zeros(N)\n",
    "temp = np.zeros(N)\n",
    "numpart = np.zeros(N)\n",
    "numimg = np.zeros(N)\n",
    "d16 = np.zeros(N)\n",
    "d50 = np.zeros(N)\n",
    "d84 = np.zeros(N)\n",
    "d95 = np.zeros(N)\n",
    "d95 = np.zeros(N)\n",
    "Vtotal = np.zeros(N)\n",
    "Solidity = np.zeros(N)\n",
    "\n",
    "# pull the data and process for each group\n",
    "\n",
    "for j in range(0,N):\n",
    "\n",
    "    pdata_group = pdata[(pdata['Image Time'] >= summary['Start [DateTime]'][j]) &\n",
    "         (pdata['Image Time'] <= summary['End [DateTime]'][j])].copy()\n",
    "    \n",
    "    pdata_group=pdata_group.drop(columns=['MeanGreyValue', 'StdDev', 'MinGreyValue', 'BX', 'BY',\n",
    "                                         'Width','Height'])\n",
    "    \n",
    "    # set particle size and put in micron\n",
    "    if(darea == 1):\n",
    "        d = np.sqrt(4*(np.array(pdata_group.Area)*muperpix**2)/np.pi)\n",
    "    else:\n",
    "        d = np.array(pdata_group.Minor)\n",
    "    \n",
    "    pdata_group['D [mu]'] = d\n",
    "\n",
    "    pdata_group.to_csv(castpath+'ProcData_1_timeseries-'+summary['Type/Location'][j]+'.csv',index=False)\n",
    "\n",
    "    depth[j] = np.mean(pdata_group['Depth [m]'])\n",
    "    zbed[j] = np.mean(pdata_group['z [m]'])\n",
    "    salinity[j] = np.mean(pdata_group['PSU'])\n",
    "    temp[j] = np.mean(pdata_group['T [Celsius]'])\n",
    "    Solidity[j]= np.mean(pdata_group['Solidity'])\n",
    "    numimg[j] = int(len(pdata_group['ImgNo'].unique()))\n",
    "\n",
    "    numpart[j]=int(len(d))\n",
    "\n",
    "    if useLISST == 1:\n",
    "        lisst_bins = pd.read_csv(lisstbins_file)\n",
    "        edges = list(lisst_bins.Lower)\n",
    "        edges.append(lisst_bins.Upper.iloc[-1])\n",
    "\n",
    "    dlog=np.log(d)\n",
    "\n",
    "    if vdist == 1:\n",
    "        dvol_uL = 1e-9*(np.pi/6)*d**3 # volume in microliters (1 micron^3 = 1e-9 microliters)\n",
    "        if useLISST == 1:\n",
    "            values, base = np.histogram(d, bins=edges, weights=dvol_uL)\n",
    "            number, base2 = np.histogram(d, bins=edges)\n",
    "            centers = np.array(lisst_bins.Median)\n",
    "            width = base[1:]-base[:-1]\n",
    "        else:\n",
    "            values, logbase = np.histogram(dlog, bins=nb, weights=dvol_uL)\n",
    "            number, logbase2 = np.histogram(dlog, bins=nb)\n",
    "            logcenters = (logbase[1:]+logbase[:-1])/2\n",
    "            base = np.exp(logbase)\n",
    "            width = base[1:]-base[:-1]\n",
    "            centers = np.exp(logcenters)\n",
    "        Vtotal[j]=sum(values)\n",
    "    else:\n",
    "        if useLISST == 1:\n",
    "            values, base = np.histogram(d, bins=edges)\n",
    "            number = values.copy()\n",
    "            centers = np.array(lisst_bins.Median)\n",
    "            width = base[1:]-base[:-1]\n",
    "            values=values*(centers)**w\n",
    "        else:\n",
    "            values, logbase = np.histogram(dlog, bins=nb)\n",
    "            number = values.copy()\n",
    "            logcenters = (logbase[1:]+logbase[:-1])/2\n",
    "            base = np.exp(logbase)\n",
    "            width = base[1:]-base[:-1]\n",
    "            centers = np.exp(logcenters)\n",
    "            values=values*(centers)**w\n",
    "    \n",
    "    valuefrac = values/np.sum(values)\n",
    "    pdf=pd.DataFrame({'d_mu':centers,'vol_or_freq':values,'fraction_total':valuefrac,'numb_particles':number})\n",
    "    pdf.to_csv(castpath+'ProcData_2_PDF-'+summary['Type/Location'][j]+'.csv',index=False)\n",
    "    \n",
    "    cumulative = np.cumsum(values) # adds up the frequencies (total is the total number of points)\n",
    "    perc = cumulative/cumulative[len(cumulative)-1]\n",
    "    percfiner = perc\n",
    "    percfiner = np.insert(percfiner,0,0)\n",
    "    \n",
    "    cdf=pd.DataFrame({'d_mu':base,'fracfiner':percfiner})\n",
    "    cdf.to_csv(castpath+'ProcData_2_CDF-'+summary['Type/Location'][j]+'.csv',index=False)\n",
    "    \n",
    "    d16[j]=np.interp(0.16,percfiner,base)\n",
    "    d50[j]=np.interp(0.5,percfiner,base)\n",
    "    d84[j]=np.interp(0.84,percfiner,base)\n",
    "    d95[j]=np.interp(0.95,percfiner,base)\n",
    "\n",
    "# build the summary data table \n",
    "\n",
    "summary['Depth [m]']= depth\n",
    "summary['z [m]'] = zbed\n",
    "summary['PSU'] = salinity\n",
    "summary['Temp [C]'] = temp\n",
    "summary['Num. Images']=numimg\n",
    "summary['Num. Particles']=numpart\n",
    "summary['D16 [mu]']=d16\n",
    "summary['D50 [mu]']=d50\n",
    "summary['D84 [mu]']=d84\n",
    "summary['D95 [mu]']=d95\n",
    "summary['Total Vol [muL]']=Vtotal\n",
    "summary['Solidity_avg']=Solidity\n",
    "summary.to_csv(castpath+'ProcData_0_GroupSummary.csv',index=False)\n",
    "display(summary)\n",
    "\n",
    "# plot the data\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1,2,figsize=(6,4))\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(7.5,3.5))\n",
    "ax1.plot(summary['PSU'], summary['z [m]'],'-o', alpha=1,label='Salinity')\n",
    "ax1.plot(summary['Temp [C]'], summary['z [m]'],'-o', alpha=1,label='Temp')\n",
    "ax1.set_xlim(0,35)\n",
    "ax1.set_ylabel('z [m]')\n",
    "ax1.set_xlabel('Salinity & Temp [PSU, C] ')\n",
    "ax1.legend(loc=3)\n",
    "\n",
    "ax2.plot(summary['D16 [mu]'], summary['z [m]'], marker='o',alpha=1,label='D16')\n",
    "ax2.plot(summary['D50 [mu]'], summary['z [m]'], marker='o',alpha=1,label='D50')\n",
    "ax2.plot(summary['D84 [mu]'], summary['z [m]'], marker='o',alpha=1,label='D84')\n",
    "ax2.plot(summary['D95 [mu]'], summary['z [m]'], marker='o',alpha=1,label='D95')\n",
    "# ax3.axhline(y = 0, color = 'k', linestyle = '-', alpha=0.5, label='bed')\n",
    "# ax3.axhline(y = totaldepth, color = 'b', linestyle = '--', label='water surface')\n",
    "# ax2.set_xlim(10,500)\n",
    "ax2.set_ylabel('z [m]')\n",
    "ax2.set_xlabel('d [micron]')\n",
    "ax2.legend()\n",
    "\n",
    "ax3.plot(summary['Num. Particles'], summary['z [m]'], '-o',color='k',alpha=1,label='data')\n",
    "ax3.axvline(x = 500, color = 'r', linestyle = '--', alpha=0.5, label='500 thresh')\n",
    "ax3.set_ylim(0,totaldepth)\n",
    "ax3.set_xlim(0,)\n",
    "ax3.set_ylabel('z [m]')\n",
    "ax3.set_xlabel('Num of particles in distribution')\n",
    "ax3.legend()\n",
    "\n",
    "fig.tight_layout();\n",
    "plt.savefig(castpath+'ProcData_3_figure-GroupSummary.pdf',bbox_inches=\"tight\", pad_inches=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
